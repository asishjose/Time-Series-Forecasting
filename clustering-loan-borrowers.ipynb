{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2232033,"sourceType":"datasetVersion","datasetId":1340957}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/asishjosekakkadan/clustering-loan-borrowers?scriptVersionId=271451989\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"This is publicly available data from LendingClub.com. Lending Club connects people who need money (borrowers) with people who have money (investors). Hopefully, as an investor you would want to invest in people who showed a profile of having a high probability of paying you back.","metadata":{}},{"cell_type":"markdown","source":"## Understanding the data","metadata":{}},{"cell_type":"markdown","source":"The data set has 9,500 loans with information on the loan structure, the borrower, and whether the loan was paid back in full. We will get rid of the target column not.fully.paid to meet the unsupervised aspect.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nloan_data = pd.read_csv(\"/kaggle/input/loan-data/loan_data.csv\")\nloan_data.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loan_data.info()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preprocessing the data","metadata":{}},{"cell_type":"code","source":"percent_missing =round(100*(loan_data.isnull().sum())/len(loan_data),2)\npercent_missing","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cleaned_data = loan_data.drop(['purpose', 'not.fully.paid'], axis=1)\ncleaned_data.info()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Outliers analysis\nOne of the weaknesses of hierarchical clustering is that it is sensitive to outliers.  The distribution of each variable is given by the boxplot.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(14, 10)) \nsns.boxplot(data = cleaned_data)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def remove_outliers(data):\n   \n    df = data.copy()\n       \n    for col in list(df.columns):\n     \n          Q1 = df[str(col)].quantile(0.05)\n          Q3 = df[str(col)].quantile(0.95)\n          IQR = Q3 - Q1\n          lower_bound = Q1 - 1.5*IQR\n          upper_bound = Q3 + 1.5*IQR\n     \n          df = df[(df[str(col)] >= lower_bound) & \n    \n            (df[str(col)] <= upper_bound)]\n       \n    return df\n\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"without_outliers = remove_outliers(cleaned_data)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(14, 10)) \nsns.boxplot(data = without_outliers)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"without_outliers.shape","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The shape of the data is now 9,319 rows and 12 columns. This means that 259 observations were outliers, which have been dropped. ","metadata":{}},{"cell_type":"markdown","source":"#### Rescale the data\nSince hierarchical clustering uses Euclidean distance, which is very sensitive to dealing with variables with different scales, itâ€™s wise to rescale all the variables before computing the distance. ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\ndata_scaler = StandardScaler()\n\nscaled_data = data_scaler.fit_transform(without_outliers)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Applying the hierarchical clustering algorithm ","metadata":{}},{"cell_type":"markdown","source":"### <u> scipy.cluster.hierarchy.linkage + dendrogram </u>\n\n","metadata":{}},{"cell_type":"code","source":"from scipy.cluster.hierarchy import linkage, dendrogram\n\ncomplete_clustering = linkage(scaled_data, method=\"complete\", metric=\"euclidean\")\naverage_clustering = linkage(scaled_data, method=\"average\", metric=\"euclidean\")\nsingle_clustering = linkage(scaled_data, method=\"single\", metric=\"euclidean\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The optimal number of clusters can be obtained by identifying the highest vertical line that does not intersect with any other clusters (horizontal line)","metadata":{}},{"cell_type":"code","source":"dendrogram(complete_clustering)\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For complete linkage, it is the blue line on the right, and it generates three clusters. ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 8)) \ndendrogram(average_clustering)\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For the average linkage, it is the first blue vertical line, and it generates two clusters.","metadata":{}},{"cell_type":"code","source":"dendrogram(single_clustering,\n          truncate_mode='lastp',\n          p=999,\n          show_leaf_counts=True)\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For the single linkage, it is the first vertical line, and it generates only one cluster. \n","metadata":{}},{"cell_type":"markdown","source":"From the above observations, the average linkage seems to be the one that provides the best clustering, as opposed to the single and complete linkage, which respectively suggests considering one cluster and three clusters. Also, the optimal cluster number of two corresponds to our prior knowledge about the dataset, which is the two types of borrowers.","metadata":{}},{"cell_type":"code","source":"from scipy.cluster.hierarchy import cut_tree\n\ncluster_labels = cut_tree(average_clustering, n_clusters=2).reshape(-1,)\nwithout_outliers['Cluster'] = cluster_labels\nsns.boxplot(x='Cluster', y='fico', data=without_outliers)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### INFERENCE\n\nFrom the above boxplot, we can observe that: \n\nBorrowers from cluster 0 have the highest credit scores.  \nWhereas borrowers from cluster 1 have lower credit scores.","metadata":{}},{"cell_type":"markdown","source":"### <u>sklearn.cluster.AgglomerativeClustering</u>","metadata":{}},{"cell_type":"code","source":"without_outliers.columns","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\n\nclust = AgglomerativeClustering(n_clusters=2, linkage='average')\nlabels = clust.fit_predict(scaled_data)\n\nwithout_outliers['Clusterr'] = labels\n\nprint(without_outliers['Clusterr'].value_counts())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(without_outliers['Cluster'].value_counts())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}